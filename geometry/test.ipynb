{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f20a8ce4200>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch gradient enabled: False\n"
     ]
    }
   ],
   "source": [
    "grad_enabled = torch.is_grad_enabled()\n",
    "print(f\"Torch gradient enabled: {grad_enabled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sae_lens\n",
    "import torch\n",
    "import datasets\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "def fetch_sae_and_model(\n",
    "    sae_name: str,\n",
    ") -> Tuple[List[sae_lens.SAE], sae_lens.HookedSAETransformer]:\n",
    "    \"\"\" fetch the specified SAE and model given the SAE name\n",
    "\n",
    "    Args:\n",
    "        sae_name (str): the name of the SAE\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[sae_lens.SAE], sae_lens.HookedSAETransformer]: the SAEs and the model\n",
    "    \"\"\"    \n",
    "    if sae_name == \"llama3.1-8b\":\n",
    "        model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "        layers = 32\n",
    "        release = \"llama_scope_lxr_8x\"\n",
    "    elif sae_name == \"pythia-70m-deduped\":\n",
    "        model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "        layers = 6\n",
    "        release = \"pythia-70m-deduped-res-sm\"\n",
    "    elif sae_name == \"gemma-2-2b\":\n",
    "        model_name = \"gemma-2-2b\"\n",
    "        layers = 26\n",
    "        release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "    saes = fetch_sae(release, layers)\n",
    "    model = fetch_model(model_name)\n",
    "    return saes, model\n",
    "\n",
    "\n",
    "def fetch_sae(release: str, layers: int) -> sae_lens.SAE:\n",
    "    saes = []\n",
    "    for layer in tqdm(range(layers)):\n",
    "        if release == \"gemma-scope-2b-pt-res-canonical\":\n",
    "            sae_id = f\"layer_{layer}/width_16k/canonical\"\n",
    "        elif release == \"llama_scope_lxr_8x\":\n",
    "            sae_id = f\"l{layer}r_8x\"\n",
    "        elif release == \"pythia-70m-deduped-res-sm\":\n",
    "            sae_id = f\"blocks.{layer}.hook_resid_post\"\n",
    "        sae = sae_lens.SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "        sae.to(dtype=torch.bfloat16)\n",
    "        saes.append(sae)\n",
    "    return saes\n",
    "\n",
    "\n",
    "def fetch_model(model_name: str) -> sae_lens.HookedSAETransformer:\n",
    "    model = sae_lens.HookedSAETransformer.from_pretrained(\n",
    "        model_name, dtype=torch.bfloat16\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.63it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "saes, model = fetch_sae_and_model(\"pythia-70m-deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")[\n",
    "                \"train\"\n",
    "            ]\n",
    "text = \"text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 36\n"
     ]
    }
   ],
   "source": [
    "ds_ratio = 1e-3\n",
    "dataset_length = int(len(dataset) * ds_ratio)\n",
    "print(f\"Dataset length: {dataset_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:01<00:00, 33.23it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "layers = 6\n",
    "release = \"pythia-70m-deduped-res-sm\"\n",
    "sae_id = \"blocks.0.hook_resid_post\"\n",
    "sae1 = sae_lens.SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "sae2 = sae_lens.SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "sae2.W_dec[:300, :].zero_()\n",
    "\n",
    "\n",
    "for idx in tqdm(range(dataset_length)):\n",
    "    example = dataset[idx]\n",
    "    tokens = model.to_tokens([example[text]], prepend_bos=True)\n",
    "    loss1, cache1 = model.run_with_cache_with_saes(\n",
    "        tokens, saes=sae1, use_error_term=False\n",
    "    )\n",
    "    model.reset_saes()\n",
    "    loss2, cache2 = model.run_with_cache_with_saes(\n",
    "        tokens, saes=sae2, use_error_term=False\n",
    "    )\n",
    "    model.reset_saes()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference: 1096.0\n"
     ]
    }
   ],
   "source": [
    "differences = []\n",
    "keys = []\n",
    "for k, _ in cache1.items():\n",
    "    keys.append(k)\n",
    "    difference = (cache1[k] - cache2[k]).sum()\n",
    "    differences.append(difference)\n",
    "print(f\"Max difference: {max(differences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Layer Difference\n",
      "0   blocks.1.mlp.hook_pre      728.0\n",
      "0   blocks.2.mlp.hook_pre      284.0\n",
      "0  blocks.2.mlp.hook_post       72.0\n",
      "0   blocks.3.mlp.hook_pre       67.5\n",
      "0  blocks.3.mlp.hook_post      53.25\n",
      "0    blocks.4.attn.hook_k       66.5\n",
      "0   blocks.4.mlp.hook_pre      556.0\n",
      "0    blocks.5.attn.hook_k      217.0\n",
      "0    blocks.5.attn.hook_v      121.0\n",
      "0   blocks.5.mlp.hook_pre     1096.0\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert differences to numpy arrays for plotting\n",
    "differences_np = [diff.to(torch.float32).cpu().numpy() for diff in differences]\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_stat = []\n",
    "for k, diff in zip(keys, differences_np):\n",
    "    if diff > 50:\n",
    "        df_stat.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Layer\": [k],\n",
    "                    \"Difference\": [diff],\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "print(pd.concat(df_stat))\n",
    "# # Plot the differences\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(x=\"Layer\", y=\"Difference\", data=pd.concat(df_stat))\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.title(\"Mean Absolute Differences Across Layers\")\n",
    "# plt.xlabel(\"Layer\")\n",
    "# plt.ylabel(\"Mean Absolute Difference\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: blocks.1.mlp.hook_pre, Difference: 728.0\n",
      "Key: blocks.2.mlp.hook_pre, Difference: 284.0\n",
      "Key: blocks.2.mlp.hook_post, Difference: 72.0\n",
      "Key: blocks.3.mlp.hook_pre, Difference: 67.5\n",
      "Key: blocks.3.mlp.hook_post, Difference: 53.25\n",
      "Key: blocks.4.attn.hook_k, Difference: 66.5\n",
      "Key: blocks.4.mlp.hook_pre, Difference: 556.0\n",
      "Key: blocks.5.attn.hook_k, Difference: 217.0\n",
      "Key: blocks.5.attn.hook_v, Difference: 121.0\n",
      "Key: blocks.5.mlp.hook_pre, Difference: 1096.0\n"
     ]
    }
   ],
   "source": [
    "for k, diff in zip(keys, differences):\n",
    "    if diff > 50:\n",
    "        print(f\"Key: {k}, Difference: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: hook_embed\n",
      "Key: blocks.0.hook_resid_pre\n",
      "Key: blocks.0.ln1.hook_scale\n",
      "Key: blocks.0.ln1.hook_normalized\n",
      "Key: blocks.0.attn.hook_q\n",
      "Key: blocks.0.attn.hook_k\n",
      "Key: blocks.0.attn.hook_v\n",
      "Key: blocks.0.attn.hook_rot_q\n",
      "Key: blocks.0.attn.hook_rot_k\n",
      "Key: blocks.0.attn.hook_attn_scores\n",
      "Key: blocks.0.attn.hook_pattern\n",
      "Key: blocks.0.attn.hook_z\n",
      "Key: blocks.0.hook_attn_out\n",
      "Key: blocks.0.ln2.hook_scale\n",
      "Key: blocks.0.ln2.hook_normalized\n",
      "Key: blocks.0.mlp.hook_pre\n",
      "Key: blocks.0.mlp.hook_post\n",
      "Key: blocks.0.hook_mlp_out\n",
      "Key: blocks.0.hook_resid_post.hook_sae_input\n",
      "Key: blocks.0.hook_resid_post.hook_sae_acts_pre\n",
      "Key: blocks.0.hook_resid_post.hook_sae_acts_post\n",
      "Key: blocks.0.hook_resid_post.hook_sae_recons\n",
      "Key: blocks.0.hook_resid_post.hook_sae_output\n",
      "Key: blocks.1.hook_resid_pre\n",
      "Key: blocks.1.ln1.hook_scale\n",
      "Key: blocks.1.ln1.hook_normalized\n",
      "Key: blocks.1.attn.hook_q\n",
      "Key: blocks.1.attn.hook_k\n",
      "Key: blocks.1.attn.hook_v\n",
      "Key: blocks.1.attn.hook_rot_q\n",
      "Key: blocks.1.attn.hook_rot_k\n",
      "Key: blocks.1.attn.hook_attn_scores\n",
      "Key: blocks.1.attn.hook_pattern\n",
      "Key: blocks.1.attn.hook_z\n",
      "Key: blocks.1.hook_attn_out\n",
      "Key: blocks.1.ln2.hook_scale\n",
      "Key: blocks.1.ln2.hook_normalized\n",
      "Key: blocks.1.mlp.hook_pre\n",
      "Key: blocks.1.mlp.hook_post\n",
      "Key: blocks.1.hook_mlp_out\n",
      "Key: blocks.1.hook_resid_post\n",
      "Key: blocks.2.hook_resid_pre\n",
      "Key: blocks.2.ln1.hook_scale\n",
      "Key: blocks.2.ln1.hook_normalized\n",
      "Key: blocks.2.attn.hook_q\n",
      "Key: blocks.2.attn.hook_k\n",
      "Key: blocks.2.attn.hook_v\n",
      "Key: blocks.2.attn.hook_rot_q\n",
      "Key: blocks.2.attn.hook_rot_k\n",
      "Key: blocks.2.attn.hook_attn_scores\n",
      "Key: blocks.2.attn.hook_pattern\n",
      "Key: blocks.2.attn.hook_z\n",
      "Key: blocks.2.hook_attn_out\n",
      "Key: blocks.2.ln2.hook_scale\n",
      "Key: blocks.2.ln2.hook_normalized\n",
      "Key: blocks.2.mlp.hook_pre\n",
      "Key: blocks.2.mlp.hook_post\n",
      "Key: blocks.2.hook_mlp_out\n",
      "Key: blocks.2.hook_resid_post\n",
      "Key: blocks.3.hook_resid_pre\n",
      "Key: blocks.3.ln1.hook_scale\n",
      "Key: blocks.3.ln1.hook_normalized\n",
      "Key: blocks.3.attn.hook_q\n",
      "Key: blocks.3.attn.hook_k\n",
      "Key: blocks.3.attn.hook_v\n",
      "Key: blocks.3.attn.hook_rot_q\n",
      "Key: blocks.3.attn.hook_rot_k\n",
      "Key: blocks.3.attn.hook_attn_scores\n",
      "Key: blocks.3.attn.hook_pattern\n",
      "Key: blocks.3.attn.hook_z\n",
      "Key: blocks.3.hook_attn_out\n",
      "Key: blocks.3.ln2.hook_scale\n",
      "Key: blocks.3.ln2.hook_normalized\n",
      "Key: blocks.3.mlp.hook_pre\n",
      "Key: blocks.3.mlp.hook_post\n",
      "Key: blocks.3.hook_mlp_out\n",
      "Key: blocks.3.hook_resid_post\n",
      "Key: blocks.4.hook_resid_pre\n",
      "Key: blocks.4.ln1.hook_scale\n",
      "Key: blocks.4.ln1.hook_normalized\n",
      "Key: blocks.4.attn.hook_q\n",
      "Key: blocks.4.attn.hook_k\n",
      "Key: blocks.4.attn.hook_v\n",
      "Key: blocks.4.attn.hook_rot_q\n",
      "Key: blocks.4.attn.hook_rot_k\n",
      "Key: blocks.4.attn.hook_attn_scores\n",
      "Key: blocks.4.attn.hook_pattern\n",
      "Key: blocks.4.attn.hook_z\n",
      "Key: blocks.4.hook_attn_out\n",
      "Key: blocks.4.ln2.hook_scale\n",
      "Key: blocks.4.ln2.hook_normalized\n",
      "Key: blocks.4.mlp.hook_pre\n",
      "Key: blocks.4.mlp.hook_post\n",
      "Key: blocks.4.hook_mlp_out\n",
      "Key: blocks.4.hook_resid_post\n",
      "Key: blocks.5.hook_resid_pre\n",
      "Key: blocks.5.ln1.hook_scale\n",
      "Key: blocks.5.ln1.hook_normalized\n",
      "Key: blocks.5.attn.hook_q\n",
      "Key: blocks.5.attn.hook_k\n",
      "Key: blocks.5.attn.hook_v\n",
      "Key: blocks.5.attn.hook_rot_q\n",
      "Key: blocks.5.attn.hook_rot_k\n",
      "Key: blocks.5.attn.hook_attn_scores\n",
      "Key: blocks.5.attn.hook_pattern\n",
      "Key: blocks.5.attn.hook_z\n",
      "Key: blocks.5.hook_attn_out\n",
      "Key: blocks.5.ln2.hook_scale\n",
      "Key: blocks.5.ln2.hook_normalized\n",
      "Key: blocks.5.mlp.hook_pre\n",
      "Key: blocks.5.mlp.hook_post\n",
      "Key: blocks.5.hook_mlp_out\n",
      "Key: blocks.5.hook_resid_post\n",
      "Key: ln_final.hook_scale\n",
      "Key: ln_final.hook_normalized\n"
     ]
    }
   ],
   "source": [
    "for k, v in cache1.items():\n",
    "    print(f\"Key: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1: check the structure of the model and see the location of the hook\n",
    "# TODO2: check the attribute methods and see its influence on the output and frequency patterns\n",
    "# TODO3: cos sim and high dim vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hook_embed: Likely used to capture or modify the embeddings of the input tokens before they are fed into the model.\n",
    "\n",
    "blocks.0.hook_resid_pre: Captures or modifies the residual connection input before any processing in the first block.\n",
    "\n",
    "blocks.0.ln1.hook_scale: Captures or modifies the scaling factor in the first layer normalization of the first block.\n",
    "\n",
    "blocks.0.ln1.hook_normalized: Captures or modifies the normalized output in the first layer normalization of the first block.\n",
    "\n",
    "blocks.0.attn.hook_q: Captures or modifies the query vectors in the attention mechanism of the first block.\n",
    "\n",
    "blocks.0.attn.hook_k: Captures or modifies the key vectors in the attention mechanism of the first block.\n",
    "\n",
    "blocks.0.attn.hook_v: Captures or modifies the value vectors in the attention mechanism of the first block.\n",
    "\n",
    "blocks.0.attn.hook_rot_q: Captures or modifies the rotated query vectors, possibly for rotary positional embeddings.\n",
    "\n",
    "blocks.0.attn.hook_rot_k: Captures or modifies the rotated key vectors, possibly for rotary positional embeddings.\n",
    "\n",
    "blocks.0.attn.hook_attn_scores: Captures or modifies the attention scores before the softmax operation.\n",
    "\n",
    "blocks.0.attn.hook_pattern: Captures or modifies the attention pattern after the softmax operation.\n",
    "\n",
    "blocks.0.attn.hook_z: Captures or modifies the output of the attention mechanism.\n",
    "\n",
    "blocks.0.hook_attn_out: Captures or modifies the output of the attention mechanism before it is added to the residual connection.\n",
    "\n",
    "blocks.0.ln2.hook_scale: Captures or modifies the scaling factor in the second layer normalization of the first block.\n",
    "\n",
    "blocks.0.ln2.hook_normalized: Captures or modifies the normalized output in the second layer normalization of the first block.\n",
    "\n",
    "blocks.0.mlp.hook_pre: Captures or modifies the input to the multi-layer perceptron (MLP) in the first block.\n",
    "\n",
    "blocks.0.mlp.hook_post: Captures or modifies the output of the MLP in the first block.\n",
    "\n",
    "blocks.0.hook_mlp_out: Captures or modifies the output of the MLP before it is added to the residual connection.\n",
    "\n",
    "blocks.0.hook_resid_post.hook_sae_input: Likely captures or modifies the input to a sub-module or sub-layer within the residual connection post-processing.\n",
    "\n",
    "blocks.0.hook_resid_post.hook_sae_acts_pre: Likely captures or modifies the activations before some specific operation within the residual connection post-processing.\n",
    "\n",
    "blocks.0.hook_resid_post.hook_sae_acts_post: Likely captures or modifies the activations after some specific operation within the residual connection post-processing.\n",
    "\n",
    "blocks.0.hook_resid_post.hook_sae_recons: Likely captures or modifies the reconstructed output within the residual connection post-processing.\n",
    "\n",
    "blocks.0.hook_resid_post.hook_sae_output: Likely captures or modifies the final output within the residual connection post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxtyping\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_cosine_similarity(\n",
    "    dict_elements_1: jaxtyping.Float[torch.Tensor, \"d_sae d_llm\"],\n",
    "    dict_elements_2: jaxtyping.Float[torch.Tensor, \"d_sae d_llm\"],\n",
    "    p: int = 2,\n",
    "    dim: int = 1,\n",
    "    normalized: bool = True,\n",
    ") -> jaxtyping.Float[torch.Tensor, \"d_llm d_llm\"]:\n",
    "    \"\"\"Get the cosine similarity between the dictionary elements.\n",
    "\n",
    "    Args:\n",
    "        dict_elements_1: The first dictionary elements.\n",
    "        dict_elements_2: The second dictionary elements.\n",
    "\n",
    "    Returns:\n",
    "        The cosine similarity between the dictionary elements.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity in pytorch\n",
    "    dict_elements_1 = dict_elements_1\n",
    "    dict_elements_2 = dict_elements_2\n",
    "\n",
    "    # Normalize the tensors\n",
    "    if normalized:\n",
    "        dict_elements_1 = torch.nn.functional.normalize(dict_elements_1, p=p, dim=dim)\n",
    "        dict_elements_2 = torch.nn.functional.normalize(dict_elements_2, p=p, dim=dim)\n",
    "\n",
    "    # Compute cosine similarity using matrix multiplication\n",
    "    cosine_sim: jaxtyping.Float[torch.Tensor, \"d_llm d_llm\"] = torch.mm(\n",
    "        dict_elements_1, dict_elements_2.T\n",
    "    )\n",
    "    # max_cosine_sim, _ = torch.max(cosine_sim, dim=1)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = get_cosine_similarity(sae1.W_dec, sae2.W_dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = cos_sim.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([    5,   458,  1453,  1992,  2020,  2278,  2463,  2914,  2979,  4667,\n",
      "         4670,  4964,  5462,  5950,  6517,  6564,  6682,  6847,  7228,  7352,\n",
      "         7415,  8234,  8304, 11573, 11772, 12553, 13534, 14263, 14278, 14568,\n",
      "        16004, 17103, 17192, 18215, 18283, 18353, 18833, 19410, 19603, 19627,\n",
      "        20284, 20570, 21315, 21357, 21660, 21793, 22545, 22865, 23125, 23302,\n",
      "        23404, 23548, 23835, 23969, 24259, 25613, 25900, 26452, 26511, 26511,\n",
      "        26793, 26824, 27340, 27882, 29236, 29325, 29326, 29463, 29813, 29921,\n",
      "        31394, 32139, 32164], device='cuda:0'), tensor([ 9124,  2278, 18833,  5950, 29813,   458, 11573, 13534, 26824, 11772,\n",
      "        23404, 27340, 21315,  1992, 21357,  7415, 14263, 23835, 24259, 26452,\n",
      "         6564, 32139, 14278,  2463,  4667, 18353,  2914,  6682,  8304, 32164,\n",
      "        29463, 23548, 22865, 19603, 20570, 12553,  1453, 29921, 18215, 23969,\n",
      "        29326, 18283,  5462,  6517, 26511, 23125, 25613, 17192, 21793, 26793,\n",
      "         4670, 17103,  6847, 19627,  7228, 22545, 29236,  7352, 21660, 27882,\n",
      "        23302,  2979,  4964, 26511, 25900, 31394, 20284, 16004,  2020, 19410,\n",
      "        29325,  8234, 14568], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "indices = torch.where(cos_sim.fill_diagonal_(0) >0.9)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9009, device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim[indices[0][0], indices[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7614],\n",
       "        [13276],\n",
       "        [ 6193],\n",
       "        [31791],\n",
       "        [ 8700],\n",
       "        [ 2773],\n",
       "        [ 1576],\n",
       "        [ 6794],\n",
       "        [ 2412],\n",
       "        [ 1303]], device='cuda:0')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3600/3600 [00:53<00:00, 67.32it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "layers = 6\n",
    "release = \"pythia-70m-deduped-res-sm\"\n",
    "sae_id = \"blocks.0.hook_resid_post\"\n",
    "sae1 = sae_lens.SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "diffs = []\n",
    "for idx in tqdm(range(dataset_length*100)):\n",
    "    example = dataset[idx]\n",
    "    tokens = model.to_tokens([example[text]], prepend_bos=True)\n",
    "    loss1, cache1 = model.run_with_cache_with_saes(\n",
    "        tokens, saes=sae1, use_error_term=False\n",
    "    )\n",
    "    diffs.append(\n",
    "        (\n",
    "            cache1[\"blocks.0.hook_resid_post.hook_sae_acts_post\"][:, :, indices[0][0]]\n",
    "            - cache1[\"blocks.0.hook_resid_post.hook_sae_acts_post\"][:, :, indices[1][0]]\n",
    "        ).abs().sum()\n",
    "    )\n",
    "    model.reset_saes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 151\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for diff in diffs:\n",
    "    if diff != 0:\n",
    "        count += 1\n",
    "print(f\"Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (cos_sim.fill_diagonal_(-100) < 0.1) & (cos_sim.fill_diagonal_(-100) > -0.1)\n",
    "min_indices = torch.where(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0,     0,     0,  ..., 32767, 32767, 32767], device='cuda:0'),\n",
       " tensor([    1,     2,     3,  ..., 32762, 32763, 32766], device='cuda:0'))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3600/3600 [00:57<00:00, 62.91it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "layers = 6\n",
    "release = \"pythia-70m-deduped-res-sm\"\n",
    "sae_id = \"blocks.0.hook_resid_post\"\n",
    "sae1 = sae_lens.SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "diffs = []\n",
    "for idx in tqdm(range(dataset_length*100)):\n",
    "    example = dataset[idx]\n",
    "    tokens = model.to_tokens([example[text]], prepend_bos=True)\n",
    "    loss1, cache1 = model.run_with_cache_with_saes(\n",
    "        tokens, saes=sae1, use_error_term=False\n",
    "    )\n",
    "    diffs.append(\n",
    "        (\n",
    "            cache1[\"blocks.0.hook_resid_post.hook_sae_acts_post\"][:, :, min_indices[0][0]]\n",
    "            - cache1[\"blocks.0.hook_resid_post.hook_sae_acts_post\"][:, :, min_indices[1][0]]\n",
    "        ).abs().sum()\n",
    "    )\n",
    "    model.reset_saes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 139\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for diff in diffs:\n",
    "    if diff != 0:\n",
    "        count += 1\n",
    "print(f\"Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saegeometry-1tp4usyN-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
