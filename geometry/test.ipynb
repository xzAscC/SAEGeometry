{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f1daedcf170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset(\"cais/mmlu\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'The cyclic subgroup of Z_24 generated by 18 has order',\n",
       " 'subject': 'abstract_algebra',\n",
       " 'choices': ['4', '8', '12', '6'],\n",
       " 'answer': 0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch gradient enabled: False\n"
     ]
    }
   ],
   "source": [
    "grad_enabled = torch.is_grad_enabled()\n",
    "print(f\"Torch gradient enabled: {grad_enabled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/huohu/Documents/code/SAEGeometry/config/saegeometry-1tp4usyN-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sae_lens\n",
    "import torch\n",
    "import datasets\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "def fetch_sae_and_model(\n",
    "    sae_name: str,\n",
    ") -> Tuple[List[sae_lens.SAE], sae_lens.HookedSAETransformer]:\n",
    "    \"\"\" fetch the specified SAE and model given the SAE name\n",
    "\n",
    "    Args:\n",
    "        sae_name (str): the name of the SAE\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[sae_lens.SAE], sae_lens.HookedSAETransformer]: the SAEs and the model\n",
    "    \"\"\"    \n",
    "    if sae_name == \"llama3.1-8b\":\n",
    "        model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "        layers = 32\n",
    "        release = \"llama_scope_lxr_8x\"\n",
    "    elif sae_name == \"pythia-70m-deduped\":\n",
    "        model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "        layers = 6\n",
    "        release = \"pythia-70m-deduped-res-sm\"\n",
    "    elif sae_name == \"gemma-2-2b\":\n",
    "        model_name = \"gemma-2-2b\"\n",
    "        layers = 26\n",
    "        release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "    saes = fetch_sae(release, layers)\n",
    "    model = fetch_model(model_name)\n",
    "    return saes, model\n",
    "\n",
    "\n",
    "def fetch_sae(release: str, layers: int) -> sae_lens.SAE:\n",
    "    saes = []\n",
    "    for layer in tqdm(range(layers)):\n",
    "        if release == \"gemma-scope-2b-pt-res-canonical\":\n",
    "            sae_id = f\"layer_{layer}/width_16k/canonical\"\n",
    "        elif release == \"llama_scope_lxr_8x\":\n",
    "            sae_id = f\"l{layer}r_8x\"\n",
    "        elif release == \"pythia-70m-deduped-res-sm\":\n",
    "            sae_id = f\"blocks.{layer}.hook_resid_post\"\n",
    "        sae = sae_lens.SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "        sae.to(dtype=torch.bfloat16)\n",
    "        saes.append(sae)\n",
    "    return saes\n",
    "\n",
    "\n",
    "def fetch_model(model_name: str) -> sae_lens.HookedSAETransformer:\n",
    "    model = sae_lens.HookedSAETransformer.from_pretrained(\n",
    "        model_name, dtype=torch.bfloat16\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.41it/s]\n",
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "saes, model = fetch_sae_and_model(\"pythia-70m-deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")[\n",
    "                \"train\"\n",
    "            ]\n",
    "text = \"text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 36\n"
     ]
    }
   ],
   "source": [
    "ds_ratio = 1e-3\n",
    "dataset_length = int(len(dataset) * ds_ratio)\n",
    "print(f\"Dataset length: {dataset_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:00<00:00, 73.63it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "layers = 6\n",
    "release = \"pythia-70m-deduped-res-sm\"\n",
    "sae_id = \"blocks.0.hook_resid_post\"\n",
    "sae1 = sae_lens.SAE.from_pretrained(release, sae_id, device=\"cuda\")[0]\n",
    "ds_ratio = 1e-3\n",
    "dataset_length = int(len(dataset) * ds_ratio)\n",
    "freqs = torch.zeros(sae1.cfg.d_sae)\n",
    "doc_len = 0\n",
    "for idx in tqdm(range(dataset_length)):\n",
    "    example = dataset[idx]\n",
    "    tokens = model.to_tokens([example[text]], prepend_bos=True)\n",
    "    loss1, cache1 = model.run_with_cache_with_saes(\n",
    "        tokens, saes=sae1, use_error_term=False\n",
    "    )\n",
    "    model.reset_saes()\n",
    "    \n",
    "    local_doc_len = cache1[f\"blocks.0.hook_resid_post.hook_sae_acts_post\"].shape[1]\n",
    "    new_doc_len = doc_len + local_doc_len\n",
    "\n",
    "    freq = (\n",
    "        cache1[\"blocks.0.hook_resid_post.hook_sae_acts_post\"]\n",
    "        > 1)[0].sum(0) / local_doc_len\n",
    "    if idx == 0:\n",
    "        freqs = freq\n",
    "    else:\n",
    "        freqs = (\n",
    "            freqs * doc_len / new_doc_len\n",
    "            + freq * local_doc_len / new_doc_len\n",
    "        )\n",
    "    doc_len = new_doc_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(freqs > 0.01).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference: 1096.0\n"
     ]
    }
   ],
   "source": [
    "differences = []\n",
    "keys = []\n",
    "for k, _ in cache1.items():\n",
    "    keys.append(k)\n",
    "    difference = (cache1[k] - cache2[k]).sum()\n",
    "    differences.append(difference)\n",
    "print(f\"Max difference: {max(differences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Layer Difference\n",
      "0   blocks.1.mlp.hook_pre      728.0\n",
      "0   blocks.2.mlp.hook_pre      284.0\n",
      "0  blocks.2.mlp.hook_post       72.0\n",
      "0   blocks.3.mlp.hook_pre       67.5\n",
      "0  blocks.3.mlp.hook_post      53.25\n",
      "0    blocks.4.attn.hook_k       66.5\n",
      "0   blocks.4.mlp.hook_pre      556.0\n",
      "0    blocks.5.attn.hook_k      217.0\n",
      "0    blocks.5.attn.hook_v      121.0\n",
      "0   blocks.5.mlp.hook_pre     1096.0\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert differences to numpy arrays for plotting\n",
    "differences_np = [diff.to(torch.float32).cpu().numpy() for diff in differences]\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_stat = []\n",
    "for k, diff in zip(keys, differences_np):\n",
    "    if diff > 50:\n",
    "        df_stat.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Layer\": [k],\n",
    "                    \"Difference\": [diff],\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "print(pd.concat(df_stat))\n",
    "# # Plot the differences\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(x=\"Layer\", y=\"Difference\", data=pd.concat(df_stat))\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.title(\"Mean Absolute Differences Across Layers\")\n",
    "# plt.xlabel(\"Layer\")\n",
    "# plt.ylabel(\"Mean Absolute Difference\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: blocks.1.mlp.hook_pre, Difference: 728.0\n",
      "Key: blocks.2.mlp.hook_pre, Difference: 284.0\n",
      "Key: blocks.2.mlp.hook_post, Difference: 72.0\n",
      "Key: blocks.3.mlp.hook_pre, Difference: 67.5\n",
      "Key: blocks.3.mlp.hook_post, Difference: 53.25\n",
      "Key: blocks.4.attn.hook_k, Difference: 66.5\n",
      "Key: blocks.4.mlp.hook_pre, Difference: 556.0\n",
      "Key: blocks.5.attn.hook_k, Difference: 217.0\n",
      "Key: blocks.5.attn.hook_v, Difference: 121.0\n",
      "Key: blocks.5.mlp.hook_pre, Difference: 1096.0\n"
     ]
    }
   ],
   "source": [
    "for k, diff in zip(keys, differences):\n",
    "    if diff > 50:\n",
    "        print(f\"Key: {k}, Difference: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: hook_embed\n",
      "Key: blocks.0.hook_resid_pre\n",
      "Key: blocks.0.ln1.hook_scale\n",
      "Key: blocks.0.ln1.hook_normalized\n",
      "Key: blocks.0.attn.hook_q\n",
      "Key: blocks.0.attn.hook_k\n",
      "Key: blocks.0.attn.hook_v\n",
      "Key: blocks.0.attn.hook_rot_q\n",
      "Key: blocks.0.attn.hook_rot_k\n",
      "Key: blocks.0.attn.hook_attn_scores\n",
      "Key: blocks.0.attn.hook_pattern\n",
      "Key: blocks.0.attn.hook_z\n",
      "Key: blocks.0.hook_attn_out\n",
      "Key: blocks.0.ln2.hook_scale\n",
      "Key: blocks.0.ln2.hook_normalized\n",
      "Key: blocks.0.mlp.hook_pre\n",
      "Key: blocks.0.mlp.hook_post\n",
      "Key: blocks.0.hook_mlp_out\n",
      "Key: blocks.0.hook_resid_post.hook_sae_input\n",
      "Key: blocks.0.hook_resid_post.hook_sae_acts_pre\n",
      "Key: blocks.0.hook_resid_post.hook_sae_acts_post\n",
      "Key: blocks.0.hook_resid_post.hook_sae_recons\n",
      "Key: blocks.0.hook_resid_post.hook_sae_output\n",
      "Key: blocks.1.hook_resid_pre\n",
      "Key: blocks.1.ln1.hook_scale\n",
      "Key: blocks.1.ln1.hook_normalized\n",
      "Key: blocks.1.attn.hook_q\n",
      "Key: blocks.1.attn.hook_k\n",
      "Key: blocks.1.attn.hook_v\n",
      "Key: blocks.1.attn.hook_rot_q\n",
      "Key: blocks.1.attn.hook_rot_k\n",
      "Key: blocks.1.attn.hook_attn_scores\n",
      "Key: blocks.1.attn.hook_pattern\n",
      "Key: blocks.1.attn.hook_z\n",
      "Key: blocks.1.hook_attn_out\n",
      "Key: blocks.1.ln2.hook_scale\n",
      "Key: blocks.1.ln2.hook_normalized\n",
      "Key: blocks.1.mlp.hook_pre\n",
      "Key: blocks.1.mlp.hook_post\n",
      "Key: blocks.1.hook_mlp_out\n",
      "Key: blocks.1.hook_resid_post\n",
      "Key: blocks.2.hook_resid_pre\n",
      "Key: blocks.2.ln1.hook_scale\n",
      "Key: blocks.2.ln1.hook_normalized\n",
      "Key: blocks.2.attn.hook_q\n",
      "Key: blocks.2.attn.hook_k\n",
      "Key: blocks.2.attn.hook_v\n",
      "Key: blocks.2.attn.hook_rot_q\n",
      "Key: blocks.2.attn.hook_rot_k\n",
      "Key: blocks.2.attn.hook_attn_scores\n",
      "Key: blocks.2.attn.hook_pattern\n",
      "Key: blocks.2.attn.hook_z\n",
      "Key: blocks.2.hook_attn_out\n",
      "Key: blocks.2.ln2.hook_scale\n",
      "Key: blocks.2.ln2.hook_normalized\n",
      "Key: blocks.2.mlp.hook_pre\n",
      "Key: blocks.2.mlp.hook_post\n",
      "Key: blocks.2.hook_mlp_out\n",
      "Key: blocks.2.hook_resid_post\n",
      "Key: blocks.3.hook_resid_pre\n",
      "Key: blocks.3.ln1.hook_scale\n",
      "Key: blocks.3.ln1.hook_normalized\n",
      "Key: blocks.3.attn.hook_q\n",
      "Key: blocks.3.attn.hook_k\n",
      "Key: blocks.3.attn.hook_v\n",
      "Key: blocks.3.attn.hook_rot_q\n",
      "Key: blocks.3.attn.hook_rot_k\n",
      "Key: blocks.3.attn.hook_attn_scores\n",
      "Key: blocks.3.attn.hook_pattern\n",
      "Key: blocks.3.attn.hook_z\n",
      "Key: blocks.3.hook_attn_out\n",
      "Key: blocks.3.ln2.hook_scale\n",
      "Key: blocks.3.ln2.hook_normalized\n",
      "Key: blocks.3.mlp.hook_pre\n",
      "Key: blocks.3.mlp.hook_post\n",
      "Key: blocks.3.hook_mlp_out\n",
      "Key: blocks.3.hook_resid_post\n",
      "Key: blocks.4.hook_resid_pre\n",
      "Key: blocks.4.ln1.hook_scale\n",
      "Key: blocks.4.ln1.hook_normalized\n",
      "Key: blocks.4.attn.hook_q\n",
      "Key: blocks.4.attn.hook_k\n",
      "Key: blocks.4.attn.hook_v\n",
      "Key: blocks.4.attn.hook_rot_q\n",
      "Key: blocks.4.attn.hook_rot_k\n",
      "Key: blocks.4.attn.hook_attn_scores\n",
      "Key: blocks.4.attn.hook_pattern\n",
      "Key: blocks.4.attn.hook_z\n",
      "Key: blocks.4.hook_attn_out\n",
      "Key: blocks.4.ln2.hook_scale\n",
      "Key: blocks.4.ln2.hook_normalized\n",
      "Key: blocks.4.mlp.hook_pre\n",
      "Key: blocks.4.mlp.hook_post\n",
      "Key: blocks.4.hook_mlp_out\n",
      "Key: blocks.4.hook_resid_post\n",
      "Key: blocks.5.hook_resid_pre\n",
      "Key: blocks.5.ln1.hook_scale\n",
      "Key: blocks.5.ln1.hook_normalized\n",
      "Key: blocks.5.attn.hook_q\n",
      "Key: blocks.5.attn.hook_k\n",
      "Key: blocks.5.attn.hook_v\n",
      "Key: blocks.5.attn.hook_rot_q\n",
      "Key: blocks.5.attn.hook_rot_k\n",
      "Key: blocks.5.attn.hook_attn_scores\n",
      "Key: blocks.5.attn.hook_pattern\n",
      "Key: blocks.5.attn.hook_z\n",
      "Key: blocks.5.hook_attn_out\n",
      "Key: blocks.5.ln2.hook_scale\n",
      "Key: blocks.5.ln2.hook_normalized\n",
      "Key: blocks.5.mlp.hook_pre\n",
      "Key: blocks.5.mlp.hook_post\n",
      "Key: blocks.5.hook_mlp_out\n",
      "Key: blocks.5.hook_resid_post\n",
      "Key: ln_final.hook_scale\n",
      "Key: ln_final.hook_normalized\n"
     ]
    }
   ],
   "source": [
    "for k, v in cache1.items():\n",
    "    print(f\"Key: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1: check the structure of the model and see the location of the hook\n",
    "# TODO2: check the attribute methods and see its influence on the output and frequency patterns\n",
    "# TODO3: cos sim and high dim vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hook_embed: Likely used to capture or modify the embeddings of the input tokens before they are fed into the model.\n",
    "\n",
    "blocks.0.hook_resid_pre: Captures or modifies the residual connection input before any processing in the first block.\n",
    "\n",
    "blocks.0.ln1.hook_scale: Captures or modifies the scaling factor in the first layer normalization of the first block.\n",
    "\n",
    "blocks.0.ln1.hook_normalized: Captures or modifies the normalized output in the first layer normalization of the first block.\n",
    "\n",
    "blocks.0.attn.hook_q: Captures or modifies the query vectors in the attention mechanism of the first block.\n",
    "\n",
    "blocks.0.attn.hook_k: Captures or modifies the key vectors in the attention mechanism of the first block.\n",
    "\n",
    "blocks.0.attn.hook_v: Captures or modifies the value vectors in the attention mechanism of the first block.\n",
    "\n",
    "blocks.0.attn.hook_rot_q: Captures or modifies the rotated query vectors, possibly for rotary positional embeddings.\n",
    "\n",
    "blocks.0.attn.hook_rot_k: Captures or modifies the rotated key vectors, possibly for rotary positional embeddings.\n",
    "\n",
    "blocks.0.attn.hook_attn_scores: Captures or modifies the attention scores before the softmax operation.\n",
    "\n",
    "blocks.0.attn.hook_pattern: Captures or modifies the attention pattern after the softmax operation.\n",
    "\n",
    "blocks.0.attn.hook_z: Captures or modifies the output of the attention mechanism.\n",
    "\n",
    "blocks.0.hook_attn_out: Captures or modifies the output of the attention mechanism before it is added to the residual connection.\n",
    "\n",
    "blocks.0.ln2.hook_scale: Captures or modifies the scaling factor in the second layer normalization of the first block.\n",
    "\n",
    "blocks.0.ln2.hook_normalized: Captures or modifies the normalized output in the second layer normalization of the first block.\n",
    "\n",
    "blocks.0.mlp.hook_pre: Captures or modifies the input to the multi-layer perceptron (MLP) in the first block.\n",
    "\n",
    "blocks.0.mlp.hook_post: Captures or modifies the output of the MLP in the first block.\n",
    "\n",
    "blocks.0.hook_mlp_out: Captures or modifies the output of the MLP before it is added to the residual connection.\n",
    "\n",
    "blocks.0.hook_resid_post.hook_sae_input: Likely captures or modifies the input to a sub-module or sub-layer within the residual connection post-processing.\n",
    "\n",
    "blocks.0.hook_resid_post.hook_sae_acts_pre: Likely captures or modifies the activations before some specific operation within the residual connection post-processing.\n",
    "\n",
    "blocks.0.hook_resid_post.hook_sae_acts_post: Likely captures or modifies the activations after some specific operation within the residual connection post-processing.\n",
    "\n",
    "blocks.0.hook_resid_post.hook_sae_recons: Likely captures or modifies the reconstructed output within the residual connection post-processing.\n",
    "\n",
    "blocks.0.hook_resid_post.hook_sae_output: Likely captures or modifies the final output within the residual connection post-processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saegeometry-1tp4usyN-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
