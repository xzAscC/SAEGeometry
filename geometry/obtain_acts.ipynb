{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/huohu/Documents/code/SAEGeometry/config/saegeometry-1tp4usyN-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]/mnt/c/Users/huohu/Documents/code/SAEGeometry/config/saegeometry-1tp4usyN-py3.12/lib/python3.12/site-packages/sae_lens/sae.py:145: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n",
      "100%|██████████| 12/12 [00:51<00:00,  4.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import sae_lens\n",
    "import torch\n",
    "import jaxtyping\n",
    "import random\n",
    "import datasets\n",
    "import plotly.colors as pc\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "def obtain_data() -> (\n",
    "    Tuple[List[sae_lens.SAE], torch.nn.Module, torch.utils.data.Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    load sae, model and dataset\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    layers = 12\n",
    "    saes = []\n",
    "    from sae_lens import SAE\n",
    "\n",
    "    release = \"gpt2-small-res-jb\"\n",
    "\n",
    "    model_name = \"gpt2-small\"\n",
    "    for layer in tqdm(range(layers)):\n",
    "        sae_id = f\"blocks.{layer}.hook_resid_pre\"\n",
    "        saes.append(\n",
    "            sae_lens.SAE.from_pretrained(release=release, sae_id=sae_id, device=device)[\n",
    "                0\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    model = sae_lens.HookedSAETransformer.from_pretrained(model_name)\n",
    "    ds = datasets.load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")[\"train\"]\n",
    "\n",
    "    return saes, model, ds\n",
    "\n",
    "saes, model, ds = obtain_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 262039/262039 [00:01<00:00, 260029.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "math_dataset = datasets.load_dataset(\"TIGER-Lab/MathInstruct\")\n",
    "CodeXGlue_dataset = datasets.load_dataset(\"BAAI/TACO\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmath_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/huohu/Documents/code/SAEGeometry/config/saegeometry-1tp4usyN-py3.12/lib/python3.12/site-packages/datasets/dataset_dict.py:78\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     74\u001b[0m available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m     split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     76\u001b[0m ]\n\u001b[1;32m     77\u001b[0m suggested_split \u001b[38;5;241m=\u001b[39m available_suggested_splits[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m available_suggested_splits \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please first select a split. For example: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`my_dataset_dictionary[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggested_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m][\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']\""
     ]
    }
   ],
   "source": [
    "math_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 367/367 [00:14<00:00, 25.39it/s]\n"
     ]
    }
   ],
   "source": [
    "nz_all = []\n",
    "doc_len = 0\n",
    "layers = 12\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "freqs = torch.zeros(layers, saes[0].cfg.d_sae).to(device)\n",
    "abl_layer = 0\n",
    "abl_times = 20\n",
    "length_ds = 100\n",
    "ds_ratio = 1e-2\n",
    "length_ds = int(len(ds) * ds_ratio)\n",
    "for idx in tqdm(range(length_ds)):\n",
    "    example = ds[idx]\n",
    "    tokens = model.to_tokens([example[\"text\"]], prepend_bos=True)\n",
    "    _, cache1 = model.run_with_cache_with_saes(tokens, saes=saes, use_error_term=False)\n",
    "    local_doc_len = cache1[f\"blocks.0.hook_resid_pre.hook_sae_acts_post\"].shape[1]\n",
    "    freq = torch.zeros_like(freqs)\n",
    "    new_doc_len = doc_len + local_doc_len\n",
    "    for layer in range(layers):\n",
    "        prompt2 = f\"blocks.{layer}.hook_resid_pre.hook_sae_acts_post\"\n",
    "        freq[layer] = (cache1[prompt2] > 1e-3)[0].sum(0) / local_doc_len    \n",
    "    if idx == 0:\n",
    "        freqs = freq\n",
    "    else:\n",
    "        freqs = (\n",
    "            freqs * doc_len / new_doc_len\n",
    "            + freq * local_doc_len / new_doc_len\n",
    "        )\n",
    "    doc_len = new_doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(freqs, \"../res/acts/wiki-gpt2-small-res-all12-acts.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:16<00:00, 15.30it/s]\n"
     ]
    }
   ],
   "source": [
    "nz_all = []\n",
    "doc_len = 0\n",
    "layers = 12\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "freqs = torch.zeros(layers, saes[0].cfg.d_sae).to(device)\n",
    "abl_layer = 0\n",
    "abl_times = 20\n",
    "length_ds = 100\n",
    "ds_ratio = 1e-2\n",
    "length_ds = int(len(CodeXGlue_dataset) * ds_ratio)\n",
    "for idx in tqdm(range(length_ds)):\n",
    "    example = CodeXGlue_dataset[idx]\n",
    "    tokens = model.to_tokens([example[\"solutions\"]], prepend_bos=True)\n",
    "    _, cache1 = model.run_with_cache_with_saes(tokens, saes=saes, use_error_term=False)\n",
    "    local_doc_len = cache1[f\"blocks.0.hook_resid_pre.hook_sae_acts_post\"].shape[1]\n",
    "    freq = torch.zeros_like(freqs)\n",
    "    new_doc_len = doc_len + local_doc_len\n",
    "    for layer in range(layers-1):\n",
    "        prompt2 = f\"blocks.{layer}.hook_resid_pre.hook_sae_acts_post\"\n",
    "        freq[layer] = (cache1[prompt2] > 1e-3)[0].sum(0) / local_doc_len    \n",
    "    if idx == 0:\n",
    "        freqs = freq\n",
    "    else:\n",
    "        freqs = (\n",
    "            freqs * doc_len / new_doc_len\n",
    "            + freq * local_doc_len / new_doc_len\n",
    "        )\n",
    "    doc_len = new_doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(freqs, \"../res/acts/code-gpt2-small-res-all12-acts.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/PoT/mathqa.json',\n",
       " 'output': 'n0 = 2.3\\nn1 = 60.0\\nn2 = 3.0\\nn3 = 75.0\\nt0 = n1 / 2.0\\nt1 = n3 - n1\\nt2 = t0 / t1\\nanswer = n2 + t2\\nprint(answer)',\n",
       " 'instruction': 'a thief steals a car at 2.30 pm and drives it at 60 kmph . the theft is discovered at 3 pm and the owner sets off in another car at 75 kmph when will he overtake the thief ? Please respond by writing a program in Python.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_dataset['train'][123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262/262 [00:10<00:00, 23.90it/s]\n"
     ]
    }
   ],
   "source": [
    "nz_all = []\n",
    "doc_len = 0\n",
    "layers = 12\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "freqs = torch.zeros(layers, saes[0].cfg.d_sae).to(device)\n",
    "abl_layer = 0\n",
    "abl_times = 20\n",
    "length_ds = 100\n",
    "ds_ratio = 1e-3\n",
    "length_ds = int(len(math_dataset['train']) * ds_ratio)\n",
    "for idx in tqdm(range(length_ds)):\n",
    "    example = math_dataset['train'][idx]\n",
    "    tokens = model.to_tokens([example[\"output\"]], prepend_bos=True)\n",
    "    _, cache1 = model.run_with_cache_with_saes(tokens, saes=saes, use_error_term=False)\n",
    "    local_doc_len = cache1[f\"blocks.0.hook_resid_pre.hook_sae_acts_post\"].shape[1]\n",
    "    freq = torch.zeros_like(freqs)\n",
    "    new_doc_len = doc_len + local_doc_len\n",
    "    for layer in range(layers-1):\n",
    "        prompt2 = f\"blocks.{layer}.hook_resid_pre.hook_sae_acts_post\"\n",
    "        freq[layer] = (cache1[prompt2] > 1e-3)[0].sum(0) / local_doc_len    \n",
    "    if idx == 0:\n",
    "        freqs = freq\n",
    "    else:\n",
    "        freqs = (\n",
    "            freqs * doc_len / new_doc_len\n",
    "            + freq * local_doc_len / new_doc_len\n",
    "        )\n",
    "    doc_len = new_doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(freqs, \"../res/acts/math-gpt2-small-res-all12-acts.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saegeometry-1tp4usyN-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
